{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of this notebook are redistributed and modified from [TorchIO tutorials](https://github.com/fepegar/torchio/tree/main/tutorials) (Transforms) as per the [Apache-2.0 license](https://github.com/fepegar/torchio/blob/main/LICENSE).\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/AFRICAI-MICCAI/model_development_2_models/blob/main/Notebooks/custom-DL-PyTorch-TorchIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-D50uSwNetB"
   },
   "source": [
    "# Hands-on: Training and evaluation pipeline using TorchIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoHXr1a4_9Ll"
   },
   "source": [
    "In this tutorial, we will train a [3D U-Net](https://link.springer.com/chapter/10.1007/978-3-319-46723-8_49) to perform brain segmentation from T1-weighted MRI using the [Information eXtraction from Images (IXI) dataset](https://brain-development.org/ixi-dataset/), a publicly available dataset of almost 600 subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conda environment\n",
    "\n",
    "It is suggested to create a conda environment for the summer school's notebooks. Please find conda installation instructions [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) (miniconda would be enough).  \n",
    "If you have not created/initialized the africai conda environment, run in a terminal from the parent  directory *model_development_1_data*:  \n",
    "> conda env create -f africai.yml  \n",
    "> conda activate africai\n",
    "\n",
    "*Other useful commands*:  \n",
    "To deactivate a conda environment \n",
    "> conda deactivate\n",
    "\n",
    "To delete a conda environment (e.g. africai conda environment, replace *ENV_NAME* with *africai*)\n",
    "> conda remove --name *ENV_NAME* --all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTorch  before installing TorchIO. It is recommended to use light-the-torch.  \n",
    "In a terminal, run:\n",
    "> pip install light-the-torch  \n",
    "> ltt install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wodgKLKzNwO-"
   },
   "source": [
    "Install some PyPI packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "zDN2HynH3Rk2"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet unet==0.7.7\n",
    "!pip install --quiet torchio==0.18.90\n",
    "!pip install --quiet torchvision\n",
    "!pip install --quiet matplotlib\n",
    "!pip install --quiet ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXkCTL7DAQaR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZpUoQ26oOQD"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "seed = 42  # for reproducibility\n",
    "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
    "num_epochs = 5\n",
    "\n",
    "# If the following values are False, the models will be downloaded and not computed\n",
    "compute_histograms = False\n",
    "train_whole_images = False\n",
    "train_patches = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G5vLwUkN1PS"
   },
   "source": [
    "Import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "b0NdJiFW3Uy7",
    "outputId": "6963edad-8704-469f-d52d-3151911e40fd"
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "plt.rcParams[\"figure.figsize\"] = 12, 6\n",
    "\n",
    "print(\"Last run on\", time.ctime())\n",
    "print(\"TorchIO version:\", tio.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMIOAv6OB2xa"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM3snbYkRmsL"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tp1QrfvHD3C"
   },
   "source": [
    "I downloaded the [IXI](https://brain-development.org/ixi-dataset/) dataset, segmented all brains using [ROBEX](https://ieeexplore.ieee.org/document/5742706), affine-registered everything to an MNI template with [NiftyReg](https://github.com/KCL-BMEIS/niftyreg), resampled everything into a space with a very large voxel spacing and funny orientation with [ITK](https://itk.org/) and uploaded a zip file to Dropbox.\n",
    "It will be our medical-images MNIST dataset.\n",
    "\n",
    "It can also be instantiated directly using [`torchio.datasets.IXITiny`](https://torchio.readthedocs.io/datasets.html#ixitiny), but we will do everything manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "!cd {data_dir} && curl --silent --output ixi_tiny.zip --location https://www.dropbox.com/s/ogxjwjxdv5mieah/ixi_tiny.zip?dl=0\n",
    "\n",
    "file_name = os.path.join(data_dir, \"ixi_tiny.zip\")\n",
    "with ZipFile(file_name, \"r\") as zips:\n",
    "    zips.extractall(os.path.join(data_dir))\n",
    "    print(\"Input data file unzipped!\")\n",
    "os.remove(file_name)\n",
    "del zips\n",
    "\n",
    "dataset_dir = os.path.join(data_dir, \"ixi_tiny\")\n",
    "print(os.listdir(dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebUKOkQlHmXR"
   },
   "source": [
    "### [`SubjectsDataset`](https://torchio.readthedocs.io/data/dataset.html)\n",
    "\n",
    "This is one of the most important classes in the library. It inherits from `torch.utils.data.Dataset`. It receives as input a list of [`torchio.Subject`](https://torchio.readthedocs.io/data/subject.html#subject) instances and an optional [`torchio.transforms.Transform`](https://torchio.readthedocs.io/transforms/transforms.html#torchio.transforms.Transform).\n",
    "\n",
    "The inputs to the subject class are instances of `torchio.Image`, such as [`torchio.ScalarImage`](https://torchio.readthedocs.io/data/image.html#scalarimage) or [`torchio.LabelMap`](https://torchio.readthedocs.io/data/image.html#labelmap). The image class will be used by the transforms to decide whether or not to perform the operation. For example, spatial transforms must apply to both, but intensity transforms must apply to scalar images only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi4v_xRe_0kG",
    "outputId": "dfa64c96-ee85-4f1f-9bac-9ea20f9d84af"
   },
   "outputs": [],
   "source": [
    "images_dir = Path(os.path.join(dataset_dir, \"image\"))\n",
    "labels_dir = Path(os.path.join(dataset_dir, \"label\"))\n",
    "image_paths = sorted(images_dir.glob(\"*.nii.gz\"))\n",
    "label_paths = sorted(labels_dir.glob(\"*.nii.gz\"))\n",
    "assert len(image_paths) == len(label_paths)\n",
    "\n",
    "subjects = []\n",
    "for image_path, label_path in zip(image_paths, label_paths):\n",
    "    subject = tio.Subject(\n",
    "        mri=tio.ScalarImage(image_path),\n",
    "        brain=tio.LabelMap(label_path),\n",
    "    )\n",
    "    subjects.append(subject)\n",
    "dataset = tio.SubjectsDataset(subjects)\n",
    "print(\"Dataset size:\", len(dataset), \"subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LUKg_6mRBFY"
   },
   "source": [
    "Let's take a look at one of the subjects in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "zzVqpgRf5eFN",
    "outputId": "abae1ef9-fbf5-4fab-d930-d86ef4cf623f"
   },
   "outputs": [],
   "source": [
    "one_subject = dataset[0]\n",
    "one_subject.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_J6kzGE5k8XK",
    "outputId": "89e155c9-2568-4d47-9b47-50a0b27025fe"
   },
   "outputs": [],
   "source": [
    "print(one_subject)\n",
    "print(one_subject.mri)\n",
    "print(one_subject.brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJLa4KMQYdRE"
   },
   "source": [
    "## [Transforms](https://torchio.readthedocs.io/transforms/transforms.html#transforms)\n",
    "\n",
    "For a full tutorial on TorchIO transforms, check out our [Data preprocessing and augmentation](https://github.com/AFRICAI-MICCAI/model_development_1_data/blob/main/Notebooks/4-5-%20Data-preprocessing-and-augmentation.ipynb) notebook.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/AFRICAI-MICCAI/model_development_1_data/blob/main/Notebooks/4-5-%20Data-preprocessing-and-augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chJa2GHra0lb"
   },
   "source": [
    "#### Normalization\n",
    "\n",
    "We will use the [`HistogramStandardization`](https://torchio.readthedocs.io/transforms/preprocessing.html#histogramstandardization) and the [`ZNormalization`](https://torchio.readthedocs.io/transforms/preprocessing.html#znormalization) transforms to normalize our images intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgMOMMulg8Lf"
   },
   "source": [
    "The images have been acquired by different MRI scanners at different hospitals. We will apply some normalization techniques so that intensities are similarly distributed and within similar ranges.\n",
    "\n",
    "Training the histogram is typically quite fast. It takes long here because of the fancy plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "kmjI8qKlZL1n",
    "outputId": "54cd729c-0547-45ba-b56b-69d7fcc808e6"
   },
   "outputs": [],
   "source": [
    "paths = image_paths\n",
    "\n",
    "def plot_histogram(axis, tensor, num_positions=100, label=None, alpha=0.05, color=None):\n",
    "    values = tensor.numpy().ravel()\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    positions = np.linspace(values.min(), values.max(), num=num_positions)\n",
    "    histogram = kernel(positions)\n",
    "    kwargs = dict(linewidth=1, color=\"black\" if color is None else color, alpha=alpha)\n",
    "    if label is not None:\n",
    "        kwargs[\"label\"] = label\n",
    "    axis.plot(positions, histogram, **kwargs)\n",
    "\n",
    "\n",
    "if compute_histograms:\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    for path in tqdm(paths):\n",
    "        tensor = tio.ScalarImage(path).data\n",
    "        if \"HH\" in path.name:\n",
    "            color = \"red\"\n",
    "        elif \"Guys\" in path.name:\n",
    "            color = \"green\"\n",
    "        elif \"IOP\" in path.name:\n",
    "            color = \"blue\"\n",
    "        plot_histogram(ax, tensor, color=color)\n",
    "    ax.set_xlim(-100, 2000)\n",
    "    ax.set_ylim(0, 0.004)\n",
    "    ax.set_title(\"Original histograms of all samples\")\n",
    "    ax.set_xlabel(\"Intensity\")\n",
    "    ax.grid()\n",
    "    graph = None\n",
    "else:\n",
    "    graph = display.Image(\n",
    "        url=\"https://www.dropbox.com/s/daqsg3udk61v65i/hist_original.png?dl=1\"\n",
    "    )\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "cce55487abc44cb0aab5c402d07a13ae",
      "636a2b42412b4f22aa5e82b53c29ddd1",
      "4ef45d6455aa4190bb3c1aa917523a4c",
      "46432020541c452baf299e1b0a2812fa",
      "ffbdd1a0a61a4b9d841417a9b6f209a4",
      "296629baa27b4c3daeb9b9d379aac78f",
      "f42f5fb891954b3badfbc854cc981384",
      "5d4c04e40b114031a39c5f77d3d23ccf",
      "f3367151d6a54599915549c78a00ea84",
      "d4b794390e4f44efb468b7866e8a8009",
      "c3c86465e6f9471bbfe39505404bc88b"
     ]
    },
    "id": "QGoCAJRTdMPB",
    "outputId": "324b5052-819f-482a-c16a-944975163032"
   },
   "outputs": [],
   "source": [
    "histogram_landmarks_path = os.path.join(data_dir, \"landmarks.npy\")\n",
    "landmarks = tio.HistogramStandardization.train(\n",
    "    image_paths,\n",
    "    output_path=histogram_landmarks_path,\n",
    ")\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "print(\"\\nTrained landmarks:\", landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "SE3mwfxsnp9G",
    "outputId": "9750969b-2e94-414e-a99d-1caf5e7cd41e"
   },
   "outputs": [],
   "source": [
    "landmarks_dict = {\"mri\": landmarks}\n",
    "histogram_transform = tio.HistogramStandardization(landmarks_dict)\n",
    "\n",
    "if compute_histograms:\n",
    "    fig, ax = plt.subplots(dpi=100)\n",
    "    for i, sample in enumerate(tqdm(dataset)):\n",
    "        standard = histogram_transform(sample)\n",
    "        tensor = standard.mri.data\n",
    "        path = str(sample.mri.path)\n",
    "        if \"HH\" in path:\n",
    "            color = \"red\"\n",
    "        elif \"Guys\" in path:\n",
    "            color = \"green\"\n",
    "        elif \"IOP\" in path:\n",
    "            color = \"blue\"\n",
    "        plot_histogram(ax, tensor, color=color)\n",
    "    ax.set_xlim(0, 150)\n",
    "    ax.set_ylim(0, 0.02)\n",
    "    ax.set_title(\"Intensity values of all samples after histogram standardization\")\n",
    "    ax.set_xlabel(\"Intensity\")\n",
    "    ax.grid()\n",
    "    graph = None\n",
    "else:\n",
    "    graph = display.Image(\n",
    "        url=\"https://www.dropbox.com/s/dqqaf78c86mrsgn/hist_standard.png?dl=1\"\n",
    "    )\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRwy0BvAqnlL"
   },
   "source": [
    "Z-standardization or normalization means that our output data will have zero mean and unit variance. We do this using means and variances computed for each image, as opposed to the whole dataset, as MRI values can vary a lot.\n",
    "\n",
    "In this example, we use only foreground values to compute the mean and variance. The foreground is approximated as all values above the mean, which typically works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "wqX8VOOYpQDb",
    "outputId": "c3d9fca8-190d-42c5-daf2-3fde66b9947a"
   },
   "outputs": [],
   "source": [
    "znorm_transform = tio.ZNormalization(masking_method=tio.ZNormalization.mean)\n",
    "\n",
    "sample = dataset[0]\n",
    "transform = tio.Compose([histogram_transform, znorm_transform])\n",
    "znormed = transform(sample)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plot_histogram(ax, znormed.mri.data, label=\"Z-normed\", alpha=1)\n",
    "ax.set_title(\"Intensity values of one sample after z-normalization\")\n",
    "ax.set_xlabel(\"Intensity\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRqAI2ucBwHK"
   },
   "source": [
    "## Training a network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-V_kHC5BvST",
    "outputId": "42c756bd-4a4c-432c-e789-72378721bbe4"
   },
   "outputs": [],
   "source": [
    "training_transform = tio.Compose(\n",
    "    [\n",
    "        tio.ToCanonical(),\n",
    "        tio.Resample(4),\n",
    "        tio.CropOrPad((48, 60, 48)),\n",
    "        tio.RandomMotion(p=0.2),\n",
    "        tio.HistogramStandardization({\"mri\": landmarks}),\n",
    "        tio.RandomBiasField(p=0.3),\n",
    "        tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
    "        tio.RandomNoise(p=0.5),\n",
    "        tio.RandomFlip(),\n",
    "        tio.OneOf(\n",
    "            {\n",
    "                tio.RandomAffine(): 0.8,\n",
    "                tio.RandomElasticDeformation(): 0.2,\n",
    "            }\n",
    "        ),\n",
    "        tio.OneHot(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transform = tio.Compose(\n",
    "    [\n",
    "        tio.ToCanonical(),\n",
    "        tio.Resample(4),\n",
    "        tio.CropOrPad((48, 60, 48)),\n",
    "        tio.HistogramStandardization({\"mri\": landmarks}),\n",
    "        tio.ZNormalization(masking_method=tio.ZNormalization.mean),\n",
    "        tio.OneHot(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_subjects = len(dataset)\n",
    "num_training_subjects = int(training_split_ratio * num_subjects)\n",
    "num_validation_subjects = num_subjects - num_training_subjects\n",
    "\n",
    "num_split_subjects = num_training_subjects, num_validation_subjects\n",
    "training_subjects, validation_subjects = torch.utils.data.random_split(\n",
    "    subjects, num_split_subjects\n",
    ")\n",
    "\n",
    "training_set = tio.SubjectsDataset(training_subjects, transform=training_transform)\n",
    "\n",
    "validation_set = tio.SubjectsDataset(\n",
    "    validation_subjects, transform=validation_transform\n",
    ")\n",
    "\n",
    "print(\"Training set:\", len(training_set), \"subjects\")\n",
    "print(\"Validation set:\", len(validation_set), \"subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKYR1N68zfTl"
   },
   "source": [
    "### Deep learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oWbWAyFNTj74"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "CHANNELS_DIMENSION = 1\n",
    "SPATIAL_DIMENSIONS = 2, 3, 4\n",
    "\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    TRAIN = \"Training\"\n",
    "    VALIDATE = \"Validation\"\n",
    "\n",
    "\n",
    "def prepare_batch(batch, device):\n",
    "    inputs = batch[\"mri\"][tio.DATA].to(device)\n",
    "    targets = batch[\"brain\"][tio.DATA].to(device)\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "def get_dice_score(output, target, epsilon=1e-9):\n",
    "    p0 = output\n",
    "    g0 = target\n",
    "    p1 = 1 - p0\n",
    "    g1 = 1 - g0\n",
    "    tp = (p0 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "    fp = (p0 * g1).sum(dim=SPATIAL_DIMENSIONS)\n",
    "    fn = (p1 * g0).sum(dim=SPATIAL_DIMENSIONS)\n",
    "    num = 2 * tp\n",
    "    denom = 2 * tp + fp + fn + epsilon\n",
    "    dice_score = num / denom\n",
    "    return dice_score\n",
    "\n",
    "\n",
    "def get_dice_loss(output, target):\n",
    "    return 1 - get_dice_score(output, target)\n",
    "\n",
    "\n",
    "def get_model_and_optimizer(device):\n",
    "    model = UNet(\n",
    "        in_channels=1,\n",
    "        out_classes=2,\n",
    "        dimensions=3,\n",
    "        num_encoding_blocks=3,\n",
    "        out_channels_first_layer=8,\n",
    "        normalization=\"batch\",\n",
    "        upsampling_type=\"linear\",\n",
    "        padding=True,\n",
    "        activation=\"PReLU\",\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters())\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def run_epoch(epoch_idx, action, loader, model, optimizer):\n",
    "    is_training = action == Action.TRAIN\n",
    "    epoch_losses = []\n",
    "    times = []\n",
    "    model.train(is_training)\n",
    "    for batch_idx, batch in enumerate(tqdm(loader)):\n",
    "        inputs, targets = prepare_batch(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            logits = model(inputs)\n",
    "            probabilities = F.softmax(logits, dim=CHANNELS_DIMENSION)\n",
    "            batch_losses = get_dice_loss(probabilities, targets)\n",
    "            batch_loss = batch_losses.mean()\n",
    "            if is_training:\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            times.append(time.time())\n",
    "            epoch_losses.append(batch_loss.item())\n",
    "    epoch_losses = np.array(epoch_losses)\n",
    "    print(f\"{action.value} mean loss: {epoch_losses.mean():0.3f}\")\n",
    "    return times, epoch_losses\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_epochs, training_loader, validation_loader, model, optimizer, weights_stem\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_losses.append(\n",
    "        run_epoch(0, Action.VALIDATE, validation_loader, model, optimizer)\n",
    "    )\n",
    "    for epoch_idx in range(1, num_epochs + 1):\n",
    "        print(\"Starting epoch\", epoch_idx)\n",
    "        train_losses.append(\n",
    "            run_epoch(epoch_idx, Action.TRAIN, training_loader, model, optimizer)\n",
    "        )\n",
    "        val_losses.append(\n",
    "            run_epoch(epoch_idx, Action.VALIDATE, validation_loader, model, optimizer)\n",
    "        )\n",
    "        torch.save(model.state_dict(), f\"{weights_stem}_epoch_{epoch_idx}.pth\")\n",
    "    return np.array(train_losses), np.array(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "yi1r0WusRHit",
    "outputId": "a7c97b0d-7830-4ba4-be0b-c6dd24b9feda"
   },
   "outputs": [],
   "source": [
    "training_instance = training_set[42]  # transform is applied inside SubjectsDataset\n",
    "training_instance.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "FqAG9FAE6jkq",
    "outputId": "66d522b8-a6b0-4078-fb6b-59d048048580"
   },
   "outputs": [],
   "source": [
    "validation_instance = validation_set[42]\n",
    "validation_instance.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvlZWhmQLy1A"
   },
   "source": [
    "### Whole images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZfCg1k4Lzk-"
   },
   "outputs": [],
   "source": [
    "training_batch_size = 16\n",
    "validation_batch_size = 2 * training_batch_size\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_set,\n",
    "    batch_size=training_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=validation_batch_size,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKzPHvIEPuOX"
   },
   "source": [
    "Visualize axial slices of one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkqoMuhjbmYv"
   },
   "outputs": [],
   "source": [
    "one_batch = next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "-YLjzK0CP6WK",
    "outputId": "818322dd-4c5e-434d-976c-bfe2e8b12815"
   },
   "outputs": [],
   "source": [
    "k = 24\n",
    "batch_mri = one_batch[\"mri\"][tio.DATA][..., k]\n",
    "batch_label = one_batch[\"brain\"][tio.DATA][:, 1:, ..., k]\n",
    "slices = torch.cat((batch_mri, batch_label))\n",
    "image_path = \"batch_whole_images.png\"\n",
    "torchvision.utils.save_image(\n",
    "    slices,\n",
    "    image_path,\n",
    "    nrow=training_batch_size // 2,\n",
    "    normalize=True,\n",
    "    scale_each=True,\n",
    "    padding=0,\n",
    ")\n",
    "display.Image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VujHc3fjP6dk"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "LFpQsUAyPxeA",
    "outputId": "dae6223c-7c00-4918-9537-323c78841534"
   },
   "outputs": [],
   "source": [
    "model, optimizer = get_model_and_optimizer(device)\n",
    "weights_path = \"whole_image_state_dict.pth\"\n",
    "if train_whole_images:\n",
    "    weights_stem = \"whole_images\"\n",
    "    train_losses, val_losses = train(\n",
    "        num_epochs, training_loader, validation_loader, model, optimizer, weights_stem\n",
    "    )\n",
    "    checkpoint = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"weights\": model.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, weights_path)\n",
    "else:\n",
    "    weights_path = \"whole_image_state_dict.pth\"\n",
    "    weights_url = \"https://github.com/TorchIO-project/torchio-data/raw/main/models/whole_images_epoch_5.pth\"\n",
    "    !curl --location --silent --output {weights_path} {weights_url}\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"weights\"])\n",
    "    train_losses, val_losses = checkpoint[\"train_losses\"], checkpoint[\"val_losses\"]\n",
    "\n",
    "\n",
    "def plot_times(axis, losses, label):\n",
    "    from datetime import datetime\n",
    "\n",
    "    times, losses = losses.transpose(1, 0, 2)\n",
    "    times = [datetime.fromtimestamp(x) for x in times.flatten()]\n",
    "    axis.plot(times, losses.flatten(), label=label)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_times(ax, train_losses, \"Training\")\n",
    "plot_times(ax, val_losses, \"Validation\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Dice loss\")\n",
    "ax.set_title(\"Training with whole images\")\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS4xOEbwhKVU"
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "U-5ZGq5UZ5QK",
    "outputId": "cd993ee5-c5ee-4c3b-ce80-0a7bfe03c874"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(validation_loader))\n",
    "model.eval()\n",
    "inputs, targets = prepare_batch(batch, device)\n",
    "FIRST = 0\n",
    "FOREGROUND = 1\n",
    "with torch.no_grad():\n",
    "    probabilities = model(inputs).softmax(dim=1)[:, FOREGROUND:].cpu()\n",
    "affine = batch[\"mri\"][tio.AFFINE][0].numpy()\n",
    "subject = tio.Subject(\n",
    "    mri=tio.ScalarImage(tensor=batch[\"mri\"][tio.DATA][FIRST], affine=affine),\n",
    "    label=tio.LabelMap(tensor=batch[\"brain\"][tio.DATA][FIRST], affine=affine),\n",
    "    predicted=tio.ScalarImage(tensor=probabilities[FIRST], affine=affine),\n",
    ")\n",
    "subject.plot(figsize=(9, 8), cmap_dict={\"predicted\": \"RdBu_r\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWPzwJUoMTM_"
   },
   "source": [
    "### Patch-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkeJS7TBm6-V"
   },
   "source": [
    "Recommended read: [Patch-based pipelines](https://torchio.readthedocs.io/patches/index.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljmMI_-Uhf7J"
   },
   "outputs": [],
   "source": [
    "training_batch_size = 32\n",
    "validation_batch_size = 2 * training_batch_size\n",
    "\n",
    "patch_size = 24\n",
    "samples_per_volume = 5\n",
    "max_queue_length = 300\n",
    "sampler = tio.data.UniformSampler(patch_size)\n",
    "\n",
    "patches_training_set = tio.Queue(\n",
    "    subjects_dataset=training_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "patches_validation_set = tio.Queue(\n",
    "    subjects_dataset=validation_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "training_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_training_set, batch_size=training_batch_size\n",
    ")\n",
    "\n",
    "validation_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_validation_set, batch_size=validation_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfV5P1VeP9HA"
   },
   "source": [
    "Visualize axial slices of one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "Fvy9DIK2YIgb",
    "outputId": "4a2f9708-7302-4db9-f022-4865e8ae4202"
   },
   "outputs": [],
   "source": [
    "one_batch = next(iter(training_loader_patches))\n",
    "k = int(patch_size // 4)\n",
    "batch_mri = one_batch[\"mri\"][tio.DATA][..., k]\n",
    "batch_label = one_batch[\"brain\"][tio.DATA][:, 1:, ..., k]\n",
    "slices = torch.cat((batch_mri, batch_label))\n",
    "image_path = \"batch_patches.png\"\n",
    "torchvision.utils.save_image(\n",
    "    slices,\n",
    "    image_path,\n",
    "    nrow=training_batch_size,\n",
    "    normalize=True,\n",
    "    scale_each=True,\n",
    ")\n",
    "display.Image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCFS5pLjQAdK"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "s9a9YNLBQGSq",
    "outputId": "fad8f205-a00f-421e-a119-89f24c0625da"
   },
   "outputs": [],
   "source": [
    "model, optimizer = get_model_and_optimizer(device)\n",
    "weights_path = \"patches_state_dict.pth\"\n",
    "\n",
    "if train_patches:\n",
    "    weights_stem = \"patches\"\n",
    "    train_losses, val_losses = train(\n",
    "        num_epochs,\n",
    "        training_loader_patches,\n",
    "        validation_loader_patches,\n",
    "        model,\n",
    "        optimizer,\n",
    "        weights_stem,\n",
    "    )\n",
    "    checkpoint = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"weights\": model.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, weights_path)\n",
    "else:\n",
    "    weights_url = \"https://github.com/TorchIO-project/torchio-data/raw/main/models/patches_epoch_5.pth\"\n",
    "    !curl --location --silent --output {weights_path} {weights_url}\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"weights\"])\n",
    "    train_losses, val_losses = checkpoint[\"train_losses\"], checkpoint[\"val_losses\"]\n",
    "fig, ax = plt.subplots()\n",
    "plot_times(ax, train_losses, \"Training\")\n",
    "plot_times(ax, val_losses, \"Validation\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Dice loss\")\n",
    "ax.set_title(\"Training with patches (subvolumes)\")\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdYZ1xThQHoK"
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "SKu8k5Z2wkSD",
    "outputId": "60d15c41-ba7c-4949-c72c-4bca8946ede2"
   },
   "outputs": [],
   "source": [
    "subject = random.choice(validation_set)\n",
    "input_tensor = sample.mri.data[0]\n",
    "patch_size = 48, 48, 48  # we can user larger patches for inference\n",
    "patch_overlap = 4, 4, 4\n",
    "grid_sampler = tio.inference.GridSampler(\n",
    "    subject,\n",
    "    patch_size,\n",
    "    patch_overlap,\n",
    ")\n",
    "patch_loader = torch.utils.data.DataLoader(\n",
    "    grid_sampler, batch_size=validation_batch_size\n",
    ")\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        inputs = patches_batch[\"mri\"][tio.DATA].to(device)\n",
    "        locations = patches_batch[tio.LOCATION]\n",
    "        probabilities = model(inputs).softmax(dim=CHANNELS_DIMENSION)\n",
    "        aggregator.add_batch(probabilities, locations)\n",
    "\n",
    "foreground = aggregator.get_output_tensor()\n",
    "affine = subject.mri.affine\n",
    "prediction = tio.ScalarImage(tensor=foreground, affine=affine)\n",
    "subject.add_image(prediction, \"prediction\")\n",
    "subject.plot(figsize=(9, 8), cmap_dict={\"prediction\": \"RdBu_r\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqyNi7QuiDWv"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "By now, you should be able to code your own training and evaluation pipeline using TorchIO to process your images before and after feeding them to the network.\n",
    "\n",
    "If you want to learn more, make sure you check [all the notebooks](https://github.com/fepegar/torchio/tree/master/examples).\n",
    "\n",
    "If you like TorchIO, please star [the repository](https://github.com/fepegar/torchio) and tell all your friends about it!\n",
    "\n",
    "And if you use TorchIO for your research, please [cite our paper](https://torchio.readthedocs.io/#credits)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "TorchIO tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "296629baa27b4c3daeb9b9d379aac78f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46432020541c452baf299e1b0a2812fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4b794390e4f44efb468b7866e8a8009",
      "placeholder": "​",
      "style": "IPY_MODEL_c3c86465e6f9471bbfe39505404bc88b",
      "value": " 566/566 [00:14&lt;00:00, 59.75it/s]"
     }
    },
    "4ef45d6455aa4190bb3c1aa917523a4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4c04e40b114031a39c5f77d3d23ccf",
      "max": 566,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3367151d6a54599915549c78a00ea84",
      "value": 566
     }
    },
    "5d4c04e40b114031a39c5f77d3d23ccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "636a2b42412b4f22aa5e82b53c29ddd1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_296629baa27b4c3daeb9b9d379aac78f",
      "placeholder": "​",
      "style": "IPY_MODEL_f42f5fb891954b3badfbc854cc981384",
      "value": "100%"
     }
    },
    "c3c86465e6f9471bbfe39505404bc88b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cce55487abc44cb0aab5c402d07a13ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_636a2b42412b4f22aa5e82b53c29ddd1",
       "IPY_MODEL_4ef45d6455aa4190bb3c1aa917523a4c",
       "IPY_MODEL_46432020541c452baf299e1b0a2812fa"
      ],
      "layout": "IPY_MODEL_ffbdd1a0a61a4b9d841417a9b6f209a4"
     }
    },
    "d4b794390e4f44efb468b7866e8a8009": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3367151d6a54599915549c78a00ea84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f42f5fb891954b3badfbc854cc981384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ffbdd1a0a61a4b9d841417a9b6f209a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
