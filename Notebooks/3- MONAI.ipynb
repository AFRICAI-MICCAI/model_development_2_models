{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "[MONAI](https://docs.monai.io/en/stable/index.html) provides many useful tools to develop AI deep learning models for various tasks in medical imaging. Additionally, we recommend using [Pytorch Lightning](https://lightning.ai/pytorch-lightning) for streamlining your experiment.\n",
    "\n",
    "This notebook demonstrates a complete workflow for classification of 2D images of the MedNIST dataset. Features demonstrated include:\n",
    "* Downloading and preparing data from MedNIST for training and validation.\n",
    "* Using MONAI Transforms to homogenous data and perform data augmentations.\n",
    "* Setup Pytorch Lightning for streamlined training and testing.\n",
    "* Using predefined convolutional neural networks from MONAI to classify the 2D images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "Many more tutorials using MONAI in other tasks, such as segmentation and registration, as well as extension to 3D can be found [here](https://github.com/Project-MONAI/tutorials/tree/main)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[pillow, tqdm]\"\n",
    "!python -c \"import sklearn\" || pip install -q \"scikit-learn\"\n",
    "!python -c \"import lightning\" || pip install -q \"lightning\"\n",
    "!python -c \"import matplotlib\" || pip install -q \"matplotlib\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.apps import download_and_extract\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.utils import first\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    AsDiscreted\n",
    ")\n",
    "from monai.visualize import matshow3d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "The MedNIST dataset was compiled by combining various subsets sourced from TCIA, the RSNA Bone Age Challenge, and the NIH Chest X-ray dataset.\n",
    "\n",
    "Dr. Bradley J. Erickson M.D., Ph.D. from the Department of Radiology at Mayo Clinic generously provides access to this dataset under the Creative Commons CC BY-SA 4.0 license.\n",
    "\n",
    "When utilizing the MedNIST dataset, it's important to provide proper attribution to the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root_dir for downloading MedNIST\n",
    "root_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/MedNIST.tar.gz\"\n",
    "md5 = \"0bc7306e7427e00ad1c5526a6677552d\"\n",
    "\n",
    "compressed_file = Path(root_dir, \"MedNIST.tar.gz\")\n",
    "data_dir = Path(root_dir, \"MedNIST\")\n",
    "if data_dir.exists:\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create readable datastructure for MONAI\n",
    "\n",
    "There are 6 folders in the dataset, AbdomenCT, BreastMRI, ChestCT, CXR, Hand, HeadCT, these also correspond to the label. Now we first are going to create a list of dictionaries to match the dataset structure typically used in MONAI. https://docs.monai.io/en/stable/data.html#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get all the classes available in the MedNIST, e.g. AbdomenCT\n",
    "class_names = sorted(x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, x)))\n",
    "class_len = len(class_names)\n",
    "class_mapping = {class_names[i]: i for i in range(class_len)}\n",
    "\n",
    "data = [\n",
    "    [{\"image\": image, \"label\": class_idx} for image in (data_dir / class_names[class_idx]).glob(\"*.jpeg\")] \n",
    "    for class_idx in range(class_len)\n",
    "    ]\n",
    "\n",
    "data = [item for sublist in data for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print the first 5 items in the data structure to see what it looks like\n",
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we have the following labels\n",
    "class_mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are working on the data, lets also create a training and test split using sklearn. Here we use a 80% train and 20% test split. Also note that we set a random state. This is important if we want to repeat our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, test_size=0.20, random_state=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "Before we can classify our images we first have to do a couple of things:\n",
    "- Loading the jpeg into memory\n",
    "- Normalize the data between the samples, here we do this by scaling the intensity values in the image\n",
    "- Change the labels to one hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ScaleIntensityd(keys=[\"image\"]),\n",
    "        AsDiscreted(keys=[\"label\"], to_onehot=class_len)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset / Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training data loader\n",
    "train_ds = Dataset(data=train, transform=transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = Dataset(data=val, transform=transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MONAI we can check the first sample in the dataloader, and visualize it. This is useful when adding new transforms, as you can see what happens if you add a certain transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = first(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matshow3d(\n",
    "    volume=data[\"image\"][:,:,:,:,np.newaxis], #This is done, as we are working with 2D images and the function is originally designed for 3D images \n",
    "    fig=None,\n",
    "    title=\"input image\",\n",
    "    figsize=(100, 100),\n",
    "    frame_dim=-1,\n",
    "    show=True,\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch lightning for model training and testing\n",
    "\n",
    "Pytorch lightning does most of the heavy lifting regarding model training and testing. We only have to adjust several things:\n",
    "- Which model to use, e.g. DenseNet121\n",
    "- Which loss function, e.g. CrossEntropyLoss\n",
    "- Set a optimizer, e.g. Adam\n",
    "- Create the training and test step\n",
    "- Calculate a metric to keep track of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedNISTModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Here we define the model used for classification, e.g. DenseNet121\n",
    "        self.model = DenseNet121(spatial_dims=2, in_channels=1, out_channels=num_classes)\n",
    "\n",
    "        # Here we can define the loss function we want to use\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Define activation function for prediction\n",
    "        self.activation_function = Activations(softmax=True)\n",
    "\n",
    "        # Set a Metric for validation set\n",
    "        self.metric = ROCAUCMetric()\n",
    "        self.best_val_AUC = 0\n",
    "        self.best_val_epoch = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # This is the training step, done at each epoch, we load the image and label\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        # We run the image forward through the model (DenseNet121)\n",
    "        y_hat = self.forward(x)\n",
    "        # We calculate the loss based on the output of the model and the true labels\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "        # We report the loss function\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # We do the first step similar to training\n",
    "        x, y = batch[\"image\"], batch[\"label\"]\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "\n",
    "        # Additionally we can calculate the metric (AUC on the ROC) for each step of the validation\n",
    "        y_hat = self.activation_function(y_hat)\n",
    "        self.metric(y_pred=y_hat, y=y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Now when a complete validation epoch has done we can report the AUC on the whole set\n",
    "        val_AUC = self.metric.aggregate()\n",
    "        self.metric.reset()\n",
    "        if val_AUC > self.best_val_AUC:\n",
    "            self.best_val_AUC = val_AUC\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current AUC: {val_AUC:.4f}\"\n",
    "            f\"\\nbest val AUC: {self.best_val_AUC:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.log(\"current epoch\", self.current_epoch, on_epoch=True)\n",
    "        self.log(\"current AUC\", val_AUC, on_epoch=True)\n",
    "        self.log(\"best AUC\", self.best_val_AUC, on_epoch=True)\n",
    "        self.log(\"at epoch\", self.best_val_epoch, on_epoch=True)\n",
    "        return val_AUC\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1 # number of epochs (whole iterations over the dataset) you would like to train for\n",
    "\n",
    "model = MedNISTModel(num_classes=class_len) # We defined class_len earlier\n",
    "trainer = pl.Trainer(max_epochs=4, accelerator=\"auto\" if torch.cuda.is_available() else \"CPU\")\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Lets try to improve the results generated by the classification model. There are many methods of boosting model performance, lets look at a few."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "First, MONAI comes with many ways to perform data augmentation. These augmentations can be done by adding random transformations. *Note* that you should never to augmentations for the test/validation part of the dataset. Augmentation transforms can be found in the transforms documentation and always start with Rand (for Random), e.g. RandRotated (randomly rotating the image). Find more here: https://docs.monai.io/en/stable/transforms.html#dictionary-transforms\n",
    "\n",
    "Most random transforms have the argument prob (probability), this is an important argument as it determines the probability of the transform to be performed (e.g. prob=1 means that the transform is always performed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ScaleIntensityd(keys=[\"image\"]),\n",
    "        AsDiscreted(keys=[\"label\"], to_onehot=class_len),\n",
    "        ## EXERCISE - add transform, e.g. RandRotated(keys=[\"image\"], prob=0.5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Val and Test transforms should not have any augmentation!\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        ScaleIntensityd(keys=[\"image\"]),\n",
    "        AsDiscreted(keys=[\"label\"], to_onehot=class_len)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training data loader\n",
    "train_ds = Dataset(data=train, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = Dataset(data=val, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, num_workers=4, pin_memory=torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good while testing augmentation to set probability to 1 and plotting the first image as done before. This will make sure that you are able to see the impact of the augmentation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = first(train_loader)\n",
    "\n",
    "matshow3d(\n",
    "    volume=data[\"image\"][:,:,:,:,np.newaxis], #This is done, as we are working with 2D images and the function is originally designed for 3D images \n",
    "    fig=None,\n",
    "    title=\"input image\",\n",
    "    figsize=(100, 100),\n",
    "    frame_dim=-1,\n",
    "    show=True,\n",
    "    cmap=\"gray\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change classification model\n",
    "\n",
    "We have now used the DenseNet121, however there are many more models available in MONAI. Tryout another model described here, https://docs.monai.io/en/stable/networks.html#nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint, when changing something in a class function, you don't have to copy the whole class function or go back and change it\n",
    "# MedNISTModel.model = ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Many different decisions we made in our classification method can be changed or have parameters we could tune. Let's try some:\n",
    "- Tuning the batch size (e.g. number of samples during each training step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint, see batch_size in train_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training for more epochs (Note don't set it to high, e.g. > 10, otherwise it might take a very long time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint, see max epochs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using another optimizer, try stochastic gradient descent instead of Adam. Note that the learning rate is also a parameter which you can tune, and is ofter larger than the one used in Adam (e.g. 0.01, instead of 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint, similar to the exercise with changing the model, you can substitute configure_optimizers(self)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
